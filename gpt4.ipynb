{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from sklearn.metrics import classification_report\n",
    "from utils import html_parsing_ncbi, html_parsing_n2c2, get_classification_report, get_digit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NER (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 NCBI-Disease Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbi_df = pd.read_csv('data/NER/NCBI-disease/test_200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_ncbi_disease(sentence: str, gpt4: bool = False, shot: int = 0) -> str:\n",
    "    \"\"\"\n",
    "        Get NER zero/one-shot prediction from GPT-3.5 or GPT-4 given a sentence in NCBI-disease dataset.\n",
    "        Input:\n",
    "            sentence: a string of sentence\n",
    "            gpt4: whether to use GPT-4 or GPT-3.5\n",
    "            shot: zero-shot (0) or one-shot (1)\n",
    "        Output:\n",
    "            a HTML string that highlights all the disease entities in the sentence\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"TASK: the task is to extract disease entities in a sentence.\"\n",
    "                       \"INPUT: the input is a sentence.\"\n",
    "                       \"OUTPUT: the output is an HTML that highlights all the disease entities in the sentence. \\\n",
    "                                The highlighting should only use HTML tags <span style=\\\"background-color: #FFFF00\\\"> and </span> and no other tags.\"\n",
    "        }\n",
    "    ]\n",
    "    if shot == 1: # the example is from NCBI-disease dataset's training split\n",
    "        prompt.append(\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"In summary , inactivation of the murine ATP7B gene produces a form of cirrhotic liver disease that resembles Wilson disease in humans and the toxic milk phenotype in the mouse . .\"\n",
    "            }\n",
    "        )\n",
    "        prompt.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": 'In summary , inactivation of the murine ATP7B gene produces a form of <span style=\"background-color: #FFFF00\">cirrhotic liver disease</span> \\\n",
    "                            that resembles <span style=\"background-color: #FFFF00\">Wilson disease</span> in humans and the toxic milk phenotype in the mouse . .'\n",
    "            }\n",
    "        )\n",
    "    prompt.append(\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": sentence\n",
    "        }\n",
    "    )\n",
    "\n",
    "    gpt = \"gpt-4-0613\" if gpt4 else \"gpt-3.5-turbo-0613\"\n",
    "\n",
    "    retries = 10 # retry at most 10 times until it succeeds\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            time_start = time.time()\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model = gpt,\n",
    "                messages = prompt,\n",
    "                temperature = 0.0, # deterministic\n",
    "                request_timeout = 60,\n",
    "            )\n",
    "            time_end = time.time()\n",
    "            return response['choices'][0]['message']['content'], time_end - time_start\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "            print(f\"Retrying... {retries} retries left\")\n",
    "            retries -= 1\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "\n",
    "    raise SystemExit(\"Max retries exceeded, exiting program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(ncbi_df), 1)):\n",
    "    ncbi_df.loc[i, 'html_gpt3.5_zero_shot'], ncbi_df.loc[i, 'gpt3.5_zero_shot_time'] = get_ner_ncbi_disease(ncbi_df.iloc[i]['text'], gpt4=False, shot=0)\n",
    "    ncbi_df.loc[i, 'html_gpt4_zero_shot'], ncbi_df.loc[i, 'gpt4_zero_shot_time'] = get_ner_ncbi_disease(ncbi_df.iloc[i]['text'], gpt4=True, shot=0)\n",
    "    ncbi_df.loc[i, 'html_gpt3.5_one_shot'], ncbi_df.loc[i, 'gpt3.5_one_shot_time'] = get_ner_ncbi_disease(ncbi_df.iloc[i]['text'], gpt4=False, shot=1)\n",
    "    ncbi_df.loc[i, 'html_gpt4_one_shot'], ncbi_df.loc[i, 'gpt4_one_shot_time'] = get_ner_ncbi_disease(ncbi_df.iloc[i]['text'], gpt4=True, shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbi_df['gt_labels'], ncbi_df['gpt3.5_zero_shot_labels'] = html_parsing_ncbi(ncbi_df, 'html_gpt3.5_zero_shot')\n",
    "_, ncbi_df['gpt4_zero_shot_labels'] = html_parsing_ncbi(ncbi_df, 'html_gpt4_zero_shot')\n",
    "_, ncbi_df['gpt3.5_one_shot_labels'] = html_parsing_ncbi(ncbi_df, 'html_gpt3.5_one_shot')\n",
    "_, ncbi_df['gpt4_one_shot_labels'] = html_parsing_ncbi(ncbi_df, 'html_gpt4_one_shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>()>,\n",
       "            {'default': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.7705627705627706,\n",
       "                          'recall': 0.52046783625731,\n",
       "                          'f1-score': 0.6212914485165794})})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_report(ncbi_df, 'gt_labels', 'gpt4_one_shot_labels', 'strict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>()>,\n",
       "            {'default': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.948051948051948,\n",
       "                          'recall': 0.6403508771929824,\n",
       "                          'f1-score': 0.7643979057591623})})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_report(ncbi_df, 'gt_labels', 'gpt4_one_shot_labels', 'lenient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average GPT-3.5 zero-shot prediction time: 6.67 seconds\n",
      "Average GPT-4 zero-shot prediction time: 5.39 seconds\n",
      "Average GPT-3.5 one-shot prediction time: 6.62 seconds\n",
      "Average GPT-4 one-shot prediction time: 5.04 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average GPT-3.5 zero-shot prediction time: {ncbi_df['gpt3.5_zero_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 zero-shot prediction time: {ncbi_df['gpt4_zero_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-3.5 one-shot prediction time: {ncbi_df['gpt3.5_one_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 one-shot prediction time: {ncbi_df['gpt4_one_shot_time'].mean():.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the inference results\n",
    "ncbi_df.to_csv('data/NER/NCBI-disease/test_200_gpt_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 2018 n2c2 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2c2_df = pd.read_csv('data/NER/2018_n2c2/test_200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_2018_n2c2(sentence: str, gpt4: bool = False, shot: int = 0) -> str:\n",
    "    \"\"\"\n",
    "        Get NER zero/one-shot prediction from GPT-3.5 or GPT-4 given a sentence in 2018 n2c2 dataset.\n",
    "        Input:\n",
    "            sentence: a string of sentence\n",
    "            gpt4: whether to use GPT-4 or GPT-3.5\n",
    "            shot: zero-shot (0) or one-shot (1)\n",
    "        Output:\n",
    "            a HTML string that highlights all the disease entities in the sentence in different colors\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"TASK: the task is to extract disease entities in a sentence. The entity type includes Form, Route, Frequency, Dosage, Strength, Duration, Reason, Ade, Drug.\"\n",
    "                       \"INPUT: the input is a sentence.\"\n",
    "                       \"OUTPUT: the output is an HTML that highlights all the disease entities in the sentence in different colors: Form(#FF0000), Route(#FFA500), Frequency(#FFFF00), Dosage(#00FF00), Strength(#0000FF), Duration(#800080), Reason(#FFC0CB), Ade(#964B00), Drug(#808080) in hex code. \\\n",
    "                                The highlighting should only use HTML tags <span style=\\\"background-color: #XXXXXX\\\"> and </span> and no other tags.\"\n",
    "        }\n",
    "    ]\n",
    "    if shot == 1:\n",
    "        prompt.append(\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"Vitamin D 400 unit Tablet Sig : Two ( 2 ) Tablet PO once a day .\"\n",
    "            }\n",
    "        )\n",
    "        prompt.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": '<span style=\"background-color: #808080\">Vitamin D</span> <span style=\"background-color: #0000FF\">400 unit</span> <span style=\"background-color: #FF0000\">Tablet</span> Sig : <span style=\"background-color: #00FF00\">Two ( 2 )</span> <span style=\"background-color: #FF0000\">Tablet</span> <span style=\"background-color: #FFA500\">PO</span> <span style=\"background-color: #FFFF00\">once a day</span> .'\n",
    "            }\n",
    "        )\n",
    "    prompt.append(\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": sentence\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    gpt = \"gpt-4-0613\" if gpt4 else \"gpt-3.5-turbo-0613\"\n",
    "\n",
    "    retries = 10 # retry at most 10 times until it succeeds\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            time_start = time.time()\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model = gpt,\n",
    "                messages = prompt,\n",
    "                temperature = 0.0, # deterministic\n",
    "                request_timeout = 60,\n",
    "            )\n",
    "            time_end = time.time()\n",
    "            return response['choices'][0]['message']['content'], time_end - time_start\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "            print(f\"Retrying... {retries} retries left\")\n",
    "            retries -= 1\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "\n",
    "    raise SystemExit(\"Max retries exceeded, exiting program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(n2c2_df), 1)):\n",
    "    n2c2_df.loc[i, 'html_gpt3.5_zero_shot'], n2c2_df.loc[i, 'gpt3.5_zero_shot_time'] = get_ner_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=False, shot=0)\n",
    "    n2c2_df.loc[i, 'html_gpt4_zero_shot'], n2c2_df.loc[i, 'gpt4_zero_shot_time'] = get_ner_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=True, shot=0)\n",
    "    n2c2_df.loc[i, 'html_gpt3.5_one_shot'], n2c2_df.loc[i, 'gpt3.5_one_shot_time'] = get_ner_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=False, shot=1)\n",
    "    n2c2_df.loc[i, 'html_gpt4_one_shot'], n2c2_df.loc[i, 'gpt4_one_shot_time'] = get_ner_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=True, shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2c2_df['gt_labels'], n2c2_df['gpt3.5_zero_shot_labels'] = html_parsing_n2c2(n2c2_df, 'html_gpt3.5_zero_shot')\n",
    "_, n2c2_df['gpt4_zero_shot_labels'] = html_parsing_n2c2(n2c2_df, 'html_gpt4_zero_shot')\n",
    "_, n2c2_df['gpt3.5_one_shot_labels'] = html_parsing_n2c2(n2c2_df, 'html_gpt3.5_one_shot')\n",
    "_, n2c2_df['gpt4_one_shot_labels'] = html_parsing_n2c2(n2c2_df, 'html_gpt4_one_shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>()>,\n",
       "            {'Form': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.9285714285714286,\n",
       "                          'recall': 0.8210526315789474,\n",
       "                          'f1-score': 0.8715083798882681}),\n",
       "             'Duration': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.35,\n",
       "                          'recall': 0.5,\n",
       "                          'f1-score': 0.4117647058823529}),\n",
       "             'Ade': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.07352941176470588,\n",
       "                          'recall': 0.7142857142857143,\n",
       "                          'f1-score': 0.13333333333333333}),\n",
       "             'Dosage': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.6323529411764706,\n",
       "                          'recall': 0.6515151515151515,\n",
       "                          'f1-score': 0.6417910447761194}),\n",
       "             'Reason': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.3888888888888889,\n",
       "                          'recall': 0.5384615384615384,\n",
       "                          'f1-score': 0.45161290322580644}),\n",
       "             'Strength': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.9102564102564102,\n",
       "                          'recall': 0.8987341772151899,\n",
       "                          'f1-score': 0.9044585987261147}),\n",
       "             'Route': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.9295774647887324,\n",
       "                          'recall': 0.868421052631579,\n",
       "                          'f1-score': 0.8979591836734694}),\n",
       "             'Frequency': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.625,\n",
       "                          'recall': 0.5921052631578947,\n",
       "                          'f1-score': 0.6081081081081081}),\n",
       "             'Drug': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.7792207792207793,\n",
       "                          'recall': 0.821917808219178,\n",
       "                          'f1-score': 0.7999999999999999})})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_report(n2c2_df, 'gt_labels', 'gpt4_one_shot_labels', 'strict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>()>,\n",
       "            {'Form': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.9761904761904762,\n",
       "                          'recall': 0.8631578947368421,\n",
       "                          'f1-score': 0.9162011173184357}),\n",
       "             'Duration': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.65,\n",
       "                          'recall': 0.9285714285714286,\n",
       "                          'f1-score': 0.7647058823529412}),\n",
       "             'Ade': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.10294117647058823,\n",
       "                          'recall': 1.0,\n",
       "                          'f1-score': 0.18666666666666665}),\n",
       "             'Dosage': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.8088235294117647,\n",
       "                          'recall': 0.8333333333333334,\n",
       "                          'f1-score': 0.8208955223880597}),\n",
       "             'Reason': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.5111111111111111,\n",
       "                          'recall': 0.7076923076923077,\n",
       "                          'f1-score': 0.5935483870967742}),\n",
       "             'Strength': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.9358974358974359,\n",
       "                          'recall': 0.9240506329113924,\n",
       "                          'f1-score': 0.9299363057324842}),\n",
       "             'Route': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 1.0,\n",
       "                          'recall': 0.9342105263157895,\n",
       "                          'f1-score': 0.9659863945578232}),\n",
       "             'Frequency': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.9444444444444444,\n",
       "                          'recall': 0.8947368421052632,\n",
       "                          'f1-score': 0.918918918918919}),\n",
       "             'Drug': defaultdict(<function ner_metrics.ner_metrics.classifcation_report.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'precision': 0.8484848484848485,\n",
       "                          'recall': 0.8949771689497716,\n",
       "                          'f1-score': 0.8711111111111112})})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_report(n2c2_df, 'gt_labels', 'gpt4_one_shot_labels', 'lenient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average GPT-3.5 zero-shot prediction time: 8.09 seconds\n",
      "Average GPT-4 zero-shot prediction time: 6.40 seconds\n",
      "Average GPT-3.5 one-shot prediction time: 8.63 seconds\n",
      "Average GPT-4 one-shot prediction time: 8.21 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average GPT-3.5 zero-shot prediction time: {n2c2_df['gpt3.5_zero_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 zero-shot prediction time: {n2c2_df['gpt4_zero_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-3.5 one-shot prediction time: {n2c2_df['gpt3.5_one_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 one-shot prediction time: {n2c2_df['gpt4_one_shot_time'].mean():.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2c2_df.to_csv('data/NER/2018_n2c2/test_200_gpt_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RE (Relation Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 2018 n2c2 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Infernece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2c2_df = pd.read_csv('data/ER/2018_n2c2/test_200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_re_2018_n2c2(sentence: str, gpt4: bool = False, shot: int = 0) -> str:\n",
    "    \"\"\"\n",
    "        Get RE zero/one-shot prediction from GPT-3.5 or GPT-4 given a sentence in 2018 n2c2 dataset.\n",
    "        Args:\n",
    "            sentence: a string of sentence\n",
    "            gpt4: whether to use GPT-4 or GPT-3.5\n",
    "            shot: zero-shot (0) or one-shot (1)\n",
    "        Output:\n",
    "            a string of predicted relation\n",
    "    \"\"\"\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"TASK: the task is to classify relations for a sentence.\"\n",
    "                       \"INPUT: the input is a sentence where the entities are labeled within [E${X}] and [E${X}/] in a sentence, where X is an integer representing an unique entity.\"\n",
    "                       \"OUTPUT: your task is to select one out of the nine types of relations ('STRENGTH-DRUG', 'ROUTE-DRUG', 'FREQUENCY-DRUG', 'FORM-DRUG', 'DOSAGE-DRUG', \\\n",
    "                               'REASON-DRUG', 'DURATION-DRUG', 'ADE-DRUG', and 'No relation').\"\n",
    "        }\n",
    "    ]\n",
    "    if shot == 1:\n",
    "        prompt.append(\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"[E2] Docusate/Sodium [E2/] ( Liquid ) 100/mg PO BID/:/PRN [E1] constipation [E1/] 4 .\"\n",
    "            }\n",
    "        )\n",
    "        prompt.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": 'REASON-DRUG'\n",
    "            }\n",
    "        )\n",
    "    prompt.append(\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": sentence\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    gpt = \"gpt-4-0613\" if gpt4 else \"gpt-3.5-turbo-0613\"\n",
    "\n",
    "    retries = 10 # retry at most 10 times until it succeeds\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            time_start = time.time()\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model = gpt,\n",
    "                messages = prompt,\n",
    "                temperature = 0.0, # deterministic\n",
    "                request_timeout = 15,\n",
    "            )\n",
    "            time_end = time.time()\n",
    "            return response['choices'][0]['message']['content'], time_end - time_start\n",
    "        except:\n",
    "            print(f\"Retrying... {retries} retries left\")\n",
    "            retries -= 1\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "\n",
    "    raise SystemExit(\"Max retries exceeded, exiting program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(n2c2_df), 1)):\n",
    "    n2c2_df.loc[i, 'gpt3.5_zero_shot'], n2c2_df.loc[i, 'gpt3.5_zero_shot_time'] = get_re_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=False, shot=0)\n",
    "    n2c2_df.loc[i, 'gpt4_zero_shot'], n2c2_df.loc[i, 'gpt4_zero_shot_time'] = get_re_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=True, shot=0)\n",
    "    n2c2_df.loc[i, 'gpt3.5_one_shot'], n2c2_df.loc[i, 'gpt3.5_one_shot_time'] = get_re_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=False, shot=1)\n",
    "    n2c2_df.loc[i, 'gpt4_one_shot'], n2c2_df.loc[i, 'gpt4_one_shot_time'] = get_re_2018_n2c2(n2c2_df.iloc[i]['text'], gpt4=True, shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of ' ' if any\n",
    "n2c2_df['gpt3.5_zero_shot'] = n2c2_df['gpt3.5_zero_shot'].apply(lambda x: x[1:-1] if \"'\" in x else x)\n",
    "n2c2_df['gpt4_zero_shot'] = n2c2_df['gpt4_zero_shot'].apply(lambda x: x[1:-1] if \"'\" in x else x)\n",
    "n2c2_df['gpt3.5_one_shot'] = n2c2_df['gpt3.5_one_shot'].apply(lambda x: x[1:-1] if \"'\" in x else x)\n",
    "n2c2_df['gpt4_one_shot'] = n2c2_df['gpt4_one_shot'].apply(lambda x: x[1:-1] if \"'\" in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get digit label while considering failed LLM outputs as 'No relation'\n",
    "n2c2_df['labels'] = n2c2_df['labels'].apply(get_digit)\n",
    "n2c2_df['gpt3.5_zero_shot_labels'] = n2c2_df['gpt3.5_zero_shot'].apply(get_digit)\n",
    "n2c2_df['gpt4_zero_shot_labels'] = n2c2_df['gpt4_zero_shot'].apply(get_digit)\n",
    "n2c2_df['gpt3.5_one_shot_labels'] = n2c2_df['gpt3.5_one_shot'].apply(get_digit)\n",
    "n2c2_df['gpt4_one_shot_labels'] = n2c2_df['gpt4_one_shot'].apply(get_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98       130\n",
      "           1       1.00      0.23      0.38        13\n",
      "           2       0.82      0.75      0.78        12\n",
      "           3       1.00      0.82      0.90        11\n",
      "           4       0.83      0.91      0.87        11\n",
      "           5       0.48      1.00      0.65        10\n",
      "           6       1.00      0.33      0.50         6\n",
      "           7       0.80      0.80      0.80         5\n",
      "           8       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.89       200\n",
      "   macro avg       0.84      0.76      0.74       200\n",
      "weighted avg       0.92      0.89      0.88       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = n2c2_df['labels'].tolist()\n",
    "y_pred = n2c2_df['gpt4_one_shot_labels'].tolist()\n",
    "print(classification_report(y_true, y_pred, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average GPT-3.5 zero-shot prediction time: 0.50 seconds\n",
      "Average GPT-4 zero-shot prediction time: 0.96 seconds\n",
      "Average GPT-3.5 one-shot prediction time: 0.47 seconds\n",
      "Average GPT-4 one-shot prediction time: 0.87 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average GPT-3.5 zero-shot prediction time: {n2c2_df['gpt3.5_zero_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 zero-shot prediction time: {n2c2_df['gpt4_zero_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-3.5 one-shot prediction time: {n2c2_df['gpt3.5_one_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 one-shot prediction time: {n2c2_df['gpt4_one_shot_time'].mean():.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the inference results\n",
    "n2c2_df.to_csv('data/ER/2018_n2c2/test_200_gpt_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 GAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gad_df = pd.read_csv('data/ER/GAD/test_200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_re_gad(sentence: str, gpt4: bool = False, shot: int = 0) -> str:\n",
    "    \"\"\"\n",
    "        Get RE zero/one-shot prediction from GPT-3.5 or GPT-4 given a sentence in GAD dataset.\n",
    "        Args:\n",
    "            sentence: a string of sentence\n",
    "            gpt4: whether to use GPT-4 or GPT-3.5\n",
    "            shot: zero-shot (0) or one-shot (1)\n",
    "        Output:\n",
    "            a string of predicted relation\n",
    "    \"\"\"\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"TASK: the task is to classify relations between a disease and a gene for a sentence.\"\n",
    "                       \"INPUT: the input is a sentence where the disease is labeled as @DISEASE$ and the gene is labeled as @GENE$ accordingly in a sentence. \"\n",
    "                       \"OUTPUT: your task is to select one out of the two types of relations (0 and 1) for the gene and disease without any explanation or other characters: \\n \\\n",
    "                                0, no relations \\n \\\n",
    "                                1, has relations\"\n",
    "        }\n",
    "    ]\n",
    "    if shot == 1:\n",
    "        prompt.append(\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"We found evidence for association between @GENE$ and COGA @DISEASE$, history of blackouts, age at first drunkenness, and level of response to alcohol.\"\n",
    "            }\n",
    "        )\n",
    "        prompt.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": '1'\n",
    "            }\n",
    "        )\n",
    "    prompt.append(\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": sentence\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    gpt = \"gpt-4-0613\" if gpt4 else \"gpt-3.5-turbo-0613\"\n",
    "\n",
    "    retries = 10 # retry at most 10 times until it succeeds\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            time_start = time.time()\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model = gpt,\n",
    "                messages = prompt,\n",
    "                temperature = 0.0, # deterministic\n",
    "                request_timeout = 15,\n",
    "            )\n",
    "            time_end = time.time()\n",
    "            return response['choices'][0]['message']['content'], time_end - time_start\n",
    "        except:\n",
    "            print(f\"Retrying... {retries} retries left\")\n",
    "            retries -= 1\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "\n",
    "    raise SystemExit(\"Max retries exceeded, exiting program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(gad_df), 1)):\n",
    "    gad_df.loc[i, 'gpt3.5_zero_shot'], gad_df.loc[i, 'gpt3.5_zero_shot_time'] = get_re_gad(gad_df.iloc[i]['text'], gpt4=False, shot=0)\n",
    "    gad_df.loc[i, 'gpt4_zero_shot'], gad_df.loc[i, 'gpt4_zero_shot_time'] = get_re_gad(gad_df.iloc[i]['text'], gpt4=True, shot=0)\n",
    "    gad_df.loc[i, 'gpt3.5_one_shot'], gad_df.loc[i, 'gpt3.5_one_shot_time'] = get_re_gad(gad_df.iloc[i]['text'], gpt4=False, shot=1)\n",
    "    gad_df.loc[i, 'gpt4_one_shot'], gad_df.loc[i, 'gpt4_one_shot_time'] = get_re_gad(gad_df.iloc[i]['text'], gpt4=True, shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert some strings to int while considering failed LLM outputs as 'No relation (0)'\n",
    "gad_df['gpt3.5_zero_shot'] = gad_df['gpt3.5_zero_shot'].apply(lambda x: int(x) if x.isdigit() else 0)\n",
    "gad_df['gpt4_zero_shot'] = gad_df['gpt4_zero_shot'].apply(lambda x: int(x) if x.isdigit() else 0)\n",
    "gad_df['gpt3.5_one_shot'] = gad_df['gpt3.5_one_shot'].apply(lambda x: int(x) if x.isdigit() else 0)\n",
    "gad_df['gpt4_one_shot'] = gad_df['gpt4_one_shot'].apply(lambda x: int(x) if x.isdigit() else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.26      0.33        98\n",
      "           1       0.50      0.72      0.59       102\n",
      "\n",
      "    accuracy                           0.49       200\n",
      "   macro avg       0.48      0.49      0.46       200\n",
      "weighted avg       0.48      0.49      0.46       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = gad_df['labels'].tolist()\n",
    "y_pred = gad_df['gpt4_one_shot'].tolist()\n",
    "print(classification_report(y_true, y_pred, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average GPT-3.5 zero-shot prediction time: 0.44 seconds\n",
      "Average GPT-4 zero-shot prediction time: 0.76 seconds\n",
      "Average GPT-3.5 one-shot prediction time: 0.44 seconds\n",
      "Average GPT-4 one-shot prediction time: 0.65 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average GPT-3.5 zero-shot prediction time: {gad_df['gpt3.5_zero_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 zero-shot prediction time: {gad_df['gpt4_zero_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-3.5 one-shot prediction time: {gad_df['gpt3.5_one_shot_time'].mean():.2f} seconds\")\n",
    "print(f\"Average GPT-4 one-shot prediction time: {gad_df['gpt4_one_shot_time'].mean():.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the inference results\n",
    "gad_df.to_csv('data/ER/GAD/test_200_gpt_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
